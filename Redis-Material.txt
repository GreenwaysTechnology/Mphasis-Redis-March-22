				Redis
..............................................................................................

What is Redis?
   Redis is an open source (BSD licensed), in-memory "data structure(List,Stack,LinkedList,Set,HashTable,Map") store

What is Data Structure?
  Organization of data based on structure.

Objective :
    - Store - write - IO
    - Process - IO , CPU
    - Report -read -IO

Store:
     1.Peristent : Permanent
       -Disk store
          -Files
             -Structured
                    -Database Management System/Relational Database System
			 -SQL-----------------------Reading,Processing,Write
             -Unstructured
                    -NOSQL Movement
		    -Big Data
     2.In Memory : not permant

In Memory Data Structure
  
   Storing data inside RAM
   Read data from RAM
   Processing data from RAM.
   Storing data in cpu register.
...........................................................................................
					NoSQL
..........................................................................................

Common Characteristics NOSQL

1.schemaless
  A schemaless database allows you to store any data

A schemaless database allows any data, structured with individual fields and structures, to be stored in the database.
Being schemaless reduces ceremony (you don't have to define schemas) and increases flexibility (you can store all sorts of data without prior definition)

The concept of "schema" also applies in-memory

Every programming language captures data in the form of "schema (plan)"

java:

class Customer {
  int id;
  String name;
  double invoiceValue;
}

Customer cust=new Customer();
cust.id =1; //storing data in memory

A class definition defines the logical fields you can use to manipulate it. This is effectively a schema.

One object can combine a schema and schemaless access.

class Customer {
  int id;
  String name;
  double invoiceValue;
}

Customer cust=new Customer();
cust.city = ""

2.Not using Relational model therory

3.Running on well on clusters 
    To enable fail over,High Avaiability

4.Open sources
...........................................................................................
Drawbacks of Relational DBs
- Impedance mismatch (In-memory(object) model of an application is different from (relational) model on disk). 

- That's why there are ORM frameworks which lead to loss of performance
- They are not designed to run efficiently on clusters

Why they are picking up now

- There is a movement away from using databases as integration points towards encapsulating databases within applications and integrating through (web) services.

- The vital factor for a change in data storage was the need to support large volumes of data by running on clusters.
- Relational DBs can be very costly to support large volume of data.
............................................................................................
				NoSQL data base Types
.............................................................................................

1.Key-value Store
2.Document based Store
3.Column Based Store
4.Graph based Store


1.Key-Value Store:
 
In this model, data is stored,processed based on the concept called "Key-value"
- Key-Value
 : based on data structure called "dictionary/map/hashmap/hashtable"

 KEY ------------>Value

 id 1000 -------> Subramanian

 id 2000--------->Murgan

Implementations:

1.Redis
2.Riak
3.Memcached
4.Hazelcast
5.Apache Ignite


2.Document based Store

A document database (also known as a document-oriented database or a document store) is a database that stores information in documents.

https://www.mongodb.com/document-databases

Document:
  Collection of information;
eg:
  SGML
  XML
  HTML
  JSON -

Implementations

1.Mongodb
2.CouchDB

3.Coloumn based store

   Colum oriented data base stores data based on columns

Hbase,Cassandra,Hypertable

4.Graph Databases
  Stores data based on datastructure called "tree/graph" model

Neo4j,Inifinte graph, OrientDB, FlockDB etc....
..............................................................................................
				  Redis - Remote Dictionary Server
..............................................................................................

It is key-value no sql data store.

Redis is polyglot data store.
  Redis can be used as database
  Redis can be used as cache solution.
  Redis can be used as "Message Broker"
..............................................................................................

How to install redis? Redis distribution

Mode of Distribution:

1.Open source - Redis.io
2.Commerical  - Redis cloud - redislabs


Redis Platform:
..............
Redis is officially available on unix,linux,bsd versions only.
redis is not available on windows.

Redis is available on windows 
-via docker.
-via third party - it is not stable.

Redis on Windows using Docker:
..............................

Lab 1 : how to start redis using docker


What is redis instance?

 Redis is a simple c program. Redis is written in c language.

What are the tools available in redis?

=>Redis server
    Process like traditional database and webservers.
  -default port 6379
  -default host localhost
  -standalone mode - by default

=>Redis client
   Software which connects redis server
 Most of the programming language who offers api to connect redis server.

Java
 Jedis
 lettuce 
 Spring boot integration.

redis-cli
  Redis command line tool to connect redis


How to start  redis server?

docker run --name some-redis redis

1:C 22 Mar 2022 13:26:47.062 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
1:C 22 Mar 2022 13:26:47.062 # Redis version=6.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
1:C 22 Mar 2022 13:26:47.062 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
1:M 22 Mar 2022 13:26:47.063 * monotonic clock: POSIX clock_gettime
1:M 22 Mar 2022 13:26:47.064 * Running mode=standalone, port=6379.
1:M 22 Mar 2022 13:26:47.064 # Server initialized
1:M 22 Mar 2022 13:26:47.064 * Ready to accept connections
  

Virtual Machine
Image - passive copy of os
container - Running copy of image

In docker world, every software is delivered as images.


How to connect redis using redis-cli?


docker exec -it my-redis redis-cli

Here we connect redis server with interactive mode

-it -interactive mode
my-redis => Name of the container
redis-cli => image/container

docker exec -it my-redis redis-cli
127.0.0.1:6379> ping
PONG
127.0.0.1:6379>
.............................................................................................
Redis Architecture : Redis Components:
......................................

Relational databases such oracle/mysql/db2 stores data in the disk file.

Redis is storing data in the main memory - RAM. 
  Redis inmemory data store.

Since redis is in memory database, there is possiblity of data loss,Redis offers a concept of snapshot.

All in memory data is replicated into disk in a file: database file.
..............................................................................................
				Redis Server and Client Communication
..............................................................................................

Redis server and clients are communicated via redis protocal, redis protocal in tern uses TCP protocal.

Application Protocal:

Client-----RESP(REdis Serialization Protocal)------------------------Redis Server
            
Network Protocal:

Client-----------TCP------------------------Redis Server


..............................................................................................
				 Redis uses -> Request-Response Model
..............................................................................................

Redis clients sends sequence of "commands" with args or without args to Redis server.

Redis Server receives the commands, process the commands and a reply is sent back to client.

Redis offers lot of commands.

1.ping
  used to test server

2.echo
  used to echo the output

3.quit
   used to close the session with server

What is command?

 Command is in nutshell , it is c api(procedure/function)
..............................................................................................
				 How to store data inside redis
.............................................................................................

In RDBMS , data is stored in tables.
table is collection of row and columns

row indicates data
column indicates identifier
id name 
1  subramanian

id = 1
name =subramanian

In Redis data is stored in RAM
In redis data is stored in the form of data structure-Dictionary/Hashtable/Map -Key - value

Redis stores data in the key value pair- Redis data models

What is Key? What can be key?
 
 Key is unique indentifier which indifies the data.
 
What can be value?
  Value can be
 »»Strings
 »»Lists
 »»Sets
 »»Sorted sets
 »»Hashes
 »»Bit arrays
 »»Streams
 »»HyperLogLogs
  
How to store simple data?

Commands:
  SET KEY VALUE
  
127.0.0.1:6379> SET message "hello"
OK

How to read /get value ?

GET key

127.0.0.1:6379> SET message "hello"
OK
127.0.0.1:6379> GET message
"hello"
127.0.0.1:6379> SET id 1
OK
127.0.0.1:6379> GET id
"1"
127.0.0.1:6379> SET name "Subramanian"
OK
127.0.0.1:6379> GET name
"Subramanian"
127.0.0.1:6379>
//////////////////////////////////////////////////////////////////////////////////////////////
				Redis schema
..............................................................................................

Redis offers the database(schema), where all data is stored.

Redis has 16 logical databases, the default database starts with 0.
Redis database name starts with numbers 0 to 15.

127.0.0.1:6379> SELECT 1
OK
127.0.0.1:6379[1]> GET name
(nil)
127.0.0.1:6379[1]> SELECT 0
OK
127.0.0.1:6379> GET name
"Ram"
.............................................................................................
				  Keys
............................................................................................

1.Keys are the primary way to access data values within Redis.

2.The majority of Redis commands operate a key or keys,so it makes an excellent place to start.

3.Key names are unique by definition.

4.Key names are binary safe in Redis.

5.This means any binary sequence can be used as a key, anything from a simple string like Foo,
numbers like 42, or 3.1415, or a binary value.

127.0.0.1:6379> SET name Subramanian
OK
127.0.0.1:6379> SET 1 100
OK
127.0.0.1:6379> GET 1
"100"
127.0.0.1:6379> GET name
"Subramanian"
127.0.0.1:6379> SET true isValid
OK
127.0.0.1:6379> GET true
"isValid"
127.0.0.1:6379>

6.They can be up to 512 megabytes in size,and that may be increased in future versions of Redis.

7.However, super long keys are generally not recommended.

8.There is a trade-off between having reasonable key names versus the amounts of memory
used to store them.

9.Within a logical database, a single flat key space exists.

10.This means all the key names occupy the same space.

11.There is no automatic separation of key names into named groups such as buckets or collections.

12.Within a logical database, the key names are unique as mentioned.
But the same key name can appear in multiple logical databases,
so logical databases do provide separation of key names.

13.Typically, for Redis users, some type of structured key name
is used, often with a colon as a separator.
 
   user:1000:followers

   user - object
   1000 - unique identifier
   followers -composed object

 store information for user 1000 follower information.

  user:1000:followers 

   user - object
   1000 - unique identifier
   followers -composed object

 store information for user 100 follower information.

In this example, you can see how a key name is constructed.

We have used a mixture of tags like user and followers,
as well as actual values like the user ID of 1000.

The exact choice of how to structure these key names is down to the development team to choose.

This is an example of a typical convention used in the Redis community.

Since a key name is a binary sequence, by implication,
it's case sensitive.

 registerusers:1000:followers
 Registerusers:1000:followers
 RegisterUsers:1000:followers
 registerUsers:1000:followers

In this example, all three key names represent different keys.
The server is simply doing a binary comparison on the key name to determine if the key exists before it's retrieved or modified.
..............................................................................................

				SET KEY
................................................................................................

SET key value [EX seconds|PX milliseconds|KEEPTTL] [NX|XX] [GET]

Set key to hold the string value. If key already holds a value, it is overwritten, regardless of its type. Any previous time to live associated with the key is discarded on successful SET operation.


The SET command supports a set of options that modify its behavior:

EX seconds -- Set the specified expire time, in seconds.
PX milliseconds -- Set the specified expire time, in milliseconds.
NX -- Only set the key if it does not already exist.
XX -- Only set the key if it already exist.
KEEPTTL -- Retain the time to live associated with the key.
GET -- Return the old value stored at key, or nil when key did not exist.



Set provides a way to store a value for a given key name.
We will discuss the additional parameters
to set later in this unit.

GET returns the value at the given key name.


GET KEY :
 Get value in the given key name.

SET customer:1000 fred

GET customer:1000

SET customer:1000 fred

Get keys names from existing database

Let's define a key for a first customer, Fred,
with the ID of 1000.

Note how Redis replies with an OK response.
We can get an individual key with the GET command.

Note that the CLI marks the response as string using
the double quotes convention.

We can now add our second key for customer Jane.

There are two commands for getting
a list of existing key names in your Redis database.
..............................................................................................
KEYS With Options: Timeout /  TTL - Time to Live
................................................

127.0.0.1:6379> SET message "hello" PX 100
OK
127.0.0.1:6379> get message
(nil)
127.0.0.1:6379> SET message "hello" PX 1000
OK
127.0.0.1:6379> get message
(nil)
127.0.0.1:6379> SET greet "hello" PX 5000
OK
127.0.0.1:6379> get message
(nil)
127.0.0.1:6379> get greet
(nil)
127.0.0.1:6379> SET greet "hello" PX 5000
OK
127.0.0.1:6379> get greet
"hello"
127.0.0.1:6379> get greet
..............................................................................................
				Key Managment commands
..............................................................................................

How to delete key?

->Manual Delete
->Auto delete

Manual Deletion:
................

DEL key [key ...]

Removes the specified keys. A key is ignored if it does not exist.

Return value
Integer reply: The number of keys that were removed.

-The DEL Command will remove the key and memory associated with the key.
-This is performed as a blocking opertion.

eg:

UnLink:
......
-With UNLINK, key is unlinked , hence the name of the  command and will no longer exists.
-The memory associated with the key value is reclaimed by an asynchronous process,so the UNLINK is a  non-blocking command.

eg:
 unlink customer:100

-We can use UNLINK to remove customer 1000.
-The return from the command is the number of keys removed in this case, 1.
-When you get a key that does not exist,nil is returned, as you can see here.

unlink customer:100
>nil
 You may have already spotted this already,but the key does not have to exist
before you can manipulate it.


There is no equivalent of create table
like you would see in relational databases.
When we execute the set command, on the key that does not exist,

it causes the key to be created, and the value to be set.

eg:
127.0.0.1:6379> Keys *
1) "user:2:name"
2) "user:1:name"
3) "name"
4) "user:3:name"
5) "id"
6) "true"
7) "1"
127.0.0.1:6379> del  1
(integer) 1
127.0.0.1:6379> Keys *
1) "user:2:name"
2) "user:1:name"
3) "name"
4) "user:3:name"
5) "id"
6) "true"
127.0.0.1:6379> unlink id
(integer) 1
127.0.0.1:6379> Keys *
1) "user:2:name"
2) "user:1:name"
3) "name"
4) "user:3:name"
5) "true"
127.0.0.1:6379>
.............................................................................................
				del vs unlink
del is blocking command
unlink is nonblocking command

.............................................................................................

However, there are times when you only want to set the value if the key already exists.

EXISTS key [key ...]

EXISTS customer:1000
127.0.0.1:6379> EXISTS customer:2000
(integer) 1
127.0.0.1:6379> EXISTS customer:9000
(integer) 0
127.0.0.1:6379>

it returns 1 for key present , 0 means no key present
.............................................................................................
				Exists and SET


Use case :
i want to set key only , if key does not exits -i want to create new only if key does not exits.
127.0.0.1:6379> EXISTS customer:9000
(integer) 0
127.0.0.1:6379> SET customer:9000 jane
OK

You could first check with the exist command to see if the key is present before using SET.

But having two operations-- the exists followed by a set--means "two round trips Redis" and possible inconsistencies between the operations.
.............................................................................................
				How to avoid sending two commands
.............................................................................................

SET key value [EX seconds|PX milliseconds|KEEPTTL] [NX|XX] [GET]

NX -  for create
XX -  for update.

127.0.0.1:6379> get user:4:name
"John"
127.0.0.1:6379> SET user:4:name  foo NX
(nil)
127.0.0.1:6379> SET user:4:name  foo XX
OK
127.0.0.1:6379> get user:4:name
"foo"
127.0.0.1:6379> SET user:5:name  foo NX
OK
127.0.0.1:6379> get user:4:name
"foo"
127.0.0.1:6379> SET user:5:name  foo NX
(nil)
127.0.0.1:6379>
.............................................................................................
 			 Timeout - Define the life of any Key - TTL(Time to Live) - Expire
..............................................................................................

Generally keys are persitable, keys will not be deleted until we set the life time of keys.

Key can be deleted manually - del and unlink
Key can be deleted by timeout 

There are two patterns

1.when key is created we can define the life time of key

 SET key value [EX seconds|PX milliseconds|KEEPTTL] [NX|XX] [GET]

2.After key defined, later we can define the timeout 

 EXPIRE key seconds [NX|XX|GT|LT]

NX – Set expiry only when the key has no expiry
XX – Set expiry only when the key has an existing expiry
GT – Set expiry only when the new expiry is greater than current one
LT – Set expiry only when the new expiry is less than current one

127.0.0.1:6379> EXPIRE user:4:name 10
(integer) 1
127.0.0.1:6379> keys *
1) "user:4:name"
2) "user:2:name"
3) "user:1:name"
4) "user:3:name"
5) "customer:9000"
127.0.0.1:6379> keys *
1) "user:4:name"
2) "user:2:name"
3) "user:1:name"
4) "user:3:name"
5) "customer:9000"
127.0.0.1:6379> keys *
1) "user:4:name"
2) "user:2:name"
3) "user:1:name"
4) "user:3:name"
5) "customer:9000"
127.0.0.1:6379> keys *
1) "user:4:name"
2) "user:2:name"
3) "user:1:name"
4) "user:3:name"
5) "customer:9000"
127.0.0.1:6379> keys *
1) "user:4:name"
2) "user:2:name"
3) "user:1:name"
4) "user:3:name"
5) "customer:9000"
127.0.0.1:6379> keys *
1) "user:4:name"
2) "user:2:name"
3) "user:1:name"
4) "user:3:name"
5) "customer:9000"
127.0.0.1:6379> keys *
1) "user:4:name"
2) "user:2:name"
3) "user:1:name"
4) "user:3:name"
5) "customer:9000"
127.0.0.1:6379> keys *
1) "user:4:name"
2) "user:2:name"
3) "user:1:name"
4) "user:3:name"
5) "customer:9000"

In the above example, we are verifying the key exitence through keys * command, but which is not recommended.

i have set key and its life time,how to know what is life of key. How long key can live still.

TTL - Get Life in seconds

PTTL -  GET LIFE in milli seconds

Returns the remaining time to live of a key that has a timeout. This introspection capability allows a Redis client to check how many seconds a given key will continue to be part of the dataset.

Integer reply: TTL in seconds, or a negative value in order to signal an error

The command returns -2 if the key does not exist.
The command returns -1 if the key exists but has no associated expire.
..............................................................................................	
				Values
..............................................................................................

How to store values and what are the different types of values?

Redis stores values in the form of datastructure.

Redis stores primitives values also.

Primitive Values

1.strings 
2.numbers

in order store values, we need data type.

Value type can be:
 »»Strings
 »»Lists
 »»Sets
 »»Sorted sets
 »»Hashes
 »»Bit arrays
 »»Streams
 »»HyperLogLogs

String:
 -store alphabets
 -store numbers
 -store binary.
no int,float datatype.

String is most fundamental basic value
It is binary safe,it can contain any type of data from numbers to images or seralized objects
String can store upto 512MB data in length

Strings are interally stores data into three forms

Int: If the value string can be converted to an integer, e.g. -2^63 ~ 2^63, Redis saves the value as an integer. This is the most efficient encoding.

Embeded String: If the size of the value string is less than or equal to 44 bytes, Redis saves the string in the same chunk of the Redis object itself. 
This is more memory efficient than the Raw String encoding. Also, it's more cache-friendly.

Raw String: Otherwise, Redis uses the raw encoding.

How to know the encoded type of key?

using Object command.

OBJECT ENCODING key

 Strings can be encoded as raw (normal string encoding) or int (strings representing integers in a 64 bit signed interval are encoded in this way in order to save space).

27.0.0.1:6379> SET Player:name Sachin
OK
127.0.0.1:6379> SET player:avg 4.5
OK
127.0.0.1:6379> SET player:score 90
OK
127.0.0.1:6379> SET Player:name "Sachin"
OK
127.0.0.1:6379> Object encoding Player:name
"embstr"
127.0.0.1:6379> Object encoding player:avg
"embstr"
127.0.0.1:6379> Object encoding Player:score
(nil)
127.0.0.1:6379> Object encoding player:score
"int"

Numerical Operations on values:
...............................
127.0.0.1:6379> INCR player:score
(integer) 91
127.0.0.1:6379> INCR player:score
(integer) 92
127.0.0.1:6379> INCR player:score
(integer) 93
127.0.0.1:6379> INCR player:score
(integer) 94
127.0.0.1:6379> INCR player:score
(integer) 95
127.0.0.1:6379> DECR player:score
(integer) 94
127.0.0.1:6379> DECR player:score
(integer) 93
127.0.0.1:6379> DECR player:score
(integer) 92
127.0.0.1:6379> INCRBYFLOAT player:avg 0.5
"5"
127.0.0.1:6379> INCRBYFLOAT player:avg 0.5
"5.5"
127.0.0.1:6379> INCRBYFLOAT player:avg 0.5
"6"
127.0.0.1:6379> INCRBYFLOAT player:avg 0.5
"6.5"
127.0.0.1:6379> INCRBYFLOAT player:avg 0.5
"7"
127.0.0.1:6379> INCRBYFLOAT player:avg 0.5
"7.5"
127.0.0.1:6379> INCR player:score
(integer) 93
127.0.0.1:6379> INCRBY player:score 6
(integer) 99
127.0.0.1:6379> INCRBY player:score 4
(integer) 103
127.0.0.1:6379> INCRBY player:score 2
(integer) 105
127.0.0.1:6379>

String Operations:
..................

String length -strlength
substring  -substr

127.0.0.1:6379> SET message  "hello how are you welcome"
OK
127.0.0.1:6379> STRLEN  message
(integer) 25
127.0.0.1:6379> GETRANGE message 0 3
"hell"
127.0.0.1:6379> GETRANGE message 5 3
""
127.0.0.1:6379> GETRANGE message 0 8
"hello how"
............................................................................................
				  List
............................................................................................

List is core datastructure, where we can add,delete,update,find elements in any order
List is used to create other applications -  stack,MessageQueues,PriorityQueue.

List stores data in the sequence of strings in the insertion order. You can add element at head or tail -  interally Lists are linked list.

Adding elements into list:

Left : head
 1,2,3,4,5,6,......

Right : tail

Stack
  - Last in first out
LPUSH
LPOP
Queue
  - First in first out
RPUSH
LPOP



127.0.0.1:6379> LPUSH message hello
(integer) 1
127.0.0.1:6379> LPUSH message world
(integer) 2
127.0.0.1:6379> LINDEX message 0
"world"
127.0.0.1:6379> LINDEX message 1
"hello"
127.0.0.1:6379> LOP message
(error) ERR unknown command `LOP`, with args beginning with: `message`,
127.0.0.1:6379> LPOP message
"world"
127.0.0.1:6379> LPOP message
"hello"
127.0.0.1:6379> LPOP message
(nil)
.............................................................................................
				  Set
.............................................................................................

Redis sets are an unordered collection of strings.
It is possible to add remove and test for elements.
Set avoids duplicates

Use cases:
You can track unique things like unique ip address visiting a given blog => How many users visited /view my blog.

//////////////////////////////////////////////////////////////////////////////////////////

Sets:
Unordered collection without duplicate values

offers mathmetical operations
1.intersection
2.difference 
3.union

Lab:
127.0.0.1:6379> SADD players:online 1
(integer) 1
127.0.0.1:6379> SADD players:online 2
(integer) 0
127.0.0.1:6379> SADD players:online 4
(integer) 1

1 -new values
0 -duplicate values

Get All players:
sscan players:online 0 MATCH *
1) "0"
2) 1) "1"
   2) "2"
   3) "4"
   4) "5"
   5) "10"
   6) "40"

How many players online??
127.0.0.1:6379> SCARD players:online

127.0.0.1:6379> SADD players:online 1
(integer) 1
127.0.0.1:6379> SADD players:online 2
(integer) 0
127.0.0.1:6379> SADD players:online 4
(integer) 1
127.0.0.1:6379> scan 0 MATCH players:online *
(error) ERR syntax error
127.0.0.1:6379> sscan 0 MATCH players:online *
(error) ERR invalid cursor
127.0.0.1:6379> sscan 0 MATCH players:online match *
(error) ERR invalid cursor
127.0.0.1:6379> sscan playlers:online 0 MATCH *
1) "0"
2) (empty array)
127.0.0.1:6379> sscan players:online 0 MATCH *
1) "0"
2) 1) "1"
   2) "2"
   3) "4"
   4) "5"
   5) "10"
127.0.0.1:6379> SADD players:online 4
(integer) 0
127.0.0.1:6379> SADD players:online 4
(integer) 0
127.0.0.1:6379> SADD players:online 40
(integer) 1
127.0.0.1:6379> SADD players:online 40
(integer) 0
127.0.0.1:6379> scan 0 MATCH players:online *
(error) ERR syntax error
127.0.0.1:6379> sscan players:online 0 MATCH *
1) "0"
2) 1) "1"
   2) "2"
   3) "4"
   4) "5"
   5) "10"
   6) "40"
127.0.0.1:6379> SCARD players:online
(integer) 6
127.0.0.1:6379> sismember players:online 1
(integer) 1
127.0.0.1:6379> sismember players:online 100
(integer) 0
127.0.0.1:6379> SCARD players:friends
(integer) 1
127.0.0.1:6379>  sinter players:online players:friends
1) "10"


Union : eleminate common elements

127.0.0.1:6379> sadd key1  a b c d e
(integer) 5
127.0.0.1:6379> sadd key2 c
(integer) 1
127.0.0.1:6379> sadd key3 f g c
(integer) 3
127.0.0.1:6379> sunion key1 key2 key3
1) "f"
2) "c"
3) "b"
4) "a"
5) "d"
6) "e"
7) "g"
127.0.0.1:6379>

//////////////////////////////////////////////////////////////////////////////////////////////
Sorted Set
Hash
BitMaps
....
Streams
HyperLogLogs
Redis Modules

Java Integration with Redis

Use cases:

1.Caching Server
2.Message Broker - Pub -Sub
3.Streaming
4.Redis as first class database like mysql
etc...

...............................................................................................

SortedSet:
  It sorts elements based on some condition.

Adding elements inside SortedSet:
127.0.0.1:6379>  ZADD tech 1 redis
(integer) 1
127.0.0.1:6379>  ZADD tech 2 mongo
(integer) 1
127.0.0.1:6379>  ZADD tech 3 mysql
(integer) 1
127.0.0.1:6379>  ZADD tech 4 mysql

How many elements are there in the SortedSet?
127.0.0.1:6379> ZCARD tech
(integer) 3
..............................................................................................
Hashes:
Hashes are used to store collections of key/value pairs. Contrast a
hash with a simple string data type where there is one value corresponding
to one key. A hash has one key, but then within that structure are more fields and values.

if you want to store data like json / table structure.

HSET key field value [field value ...]

127.0.0.1:6379> HSET user:1  firstName "subramanian" lastName "Murugan" city "coimbatore" status available
(integer) 4
127.0.0.1:6379> HGETALL user:1
1) "firstName"
2) "subramanian"
3) "lastName"
4) "Murugan"
5) "city"
6) "coimbatore"
7) "status"
8) "available"
127.0.0.1:6379> HGET user:1 firstName
"subramanian"
127.0.0.1:6379> HGET user:1 lastName
"Murugan"
127.0.0.1:6379> HGET user:1 city
"coimbatore"
127.0.0.1:6379> HGET user:1 status
"available"
127.0.0.1:6379> HSET user:1  firstName "Ram"
(integer) 0
127.0.0.1:6379> HSET user:1  country "India"
(integer) 1
127.0.0.1:6379> HGETALL user:1
 1) "firstName"
 2) "Ram"
 3) "lastName"
 4) "Murugan"
 5) "city"
 6) "coimbatore"
 7) "status"
 8) "available"
 9) "country"
10) "India"
127.0.0.1:6379> HMSET user:1 country "USA"
OK
127.0.0.1:6379> HGETALL user:1
 1) "firstName"
 2) "Ram"
 3) "lastName"
 4) "Murugan"
 5) "city"
 6) "coimbatore"
 7) "status"
 8) "available"
 9) "country"
10) "USA"
127.0.0.1:6379> HMSET user:1 state "Newyork"
OK
127.0.0.1:6379> HGETALL user:1
 1) "firstName"
 2) "Ram"
 3) "lastName"
 4) "Murugan"
 5) "city"
 6) "coimbatore"
 7) "status"
 8) "available"
 9) "country"
10) "USA"
11) "state"
12) "Newyork"
127.0.0.1:6379> HDEL user:1 state
(integer) 1
127.0.0.1:6379> HDEL user:1 state
(integer) 0
127.0.0.1:6379> HMSET user:1 rank 10
OK
127.0.0.1:6379> HGETALL user:1
 1) "firstName"
 2) "Ram"
 3) "lastName"
 4) "Murugan"
 5) "city"
 6) "coimbatore"
 7) "status"
 8) "available"
 9) "country"
10) "USA"
11) "rank"
12) "10"
127.0.0.1:6379> HINCRBY user:1 rank 5
(integer) 15
127.0.0.1:6379> HINCRBY user:1 rank 5
(integer) 20
127.0.0.1:6379> HGETALL user:1
 1) "firstName"
 2) "Ram"
 3) "lastName"
 4) "Murugan"
 5) "city"
 6) "coimbatore"
 7) "status"
 8) "available"
 9) "country"
10) "USA"
11) "rank"
12) "20"
127.0.0.1:6379> HHKEYS user:1
(error) ERR unknown command `HHKEYS`, with args beginning with: `user:1`,
127.0.0.1:6379> HKEYS user:1
1) "firstName"
2) "lastName"
3) "city"
4) "status"
5) "country"
6) "rank"
127.0.0.1:6379>
..............................................................................................
					Bit Maps
.............................................................................................

login 
  foo
  foo@123

28-03-22 - login ...logout... login...logout..
29-03-2 --
bitmaps:
BitMaps are  a data type used within Redis and represents a long list of bits that contain 0 by default and we can use SETBIT Command to flip to 1 or 0


Key                    Value                               type
	
a_bitmap     0 0 0  0 0 0 0 0 0 0 0 0 0 0                  binary string

bitmap can store up to 2pow 32 bits,about 4 billion items

Use case: login use case

SETBIT logins:2017:04 6 1

here login:2017:04 is key
6 is offset , typically userid as offset
1 is active bit

SETBIT logins:2017:04 6 1s


27.0.0.1:6379> SETBIT mon  1 1
(integer) 0
127.0.0.1:6379> SETBIT mon  2 0
(integer) 0
127.0.0.1:6379> GETBIT mon 1
(integer) 1
127.0.0.1:6379> GETBIT mon 2
(integer) 0
...........................................................................................
				  Redis Java Applications
............................................................................................

Redis java Clients:

1.Redisson 
2.Jedis
3.Lettuce


Redisson :
=>Redisson is a Redis Java client with features of In-Memory Data Grid. It provides more convenient and easiest way to work with Redis. Redisson objects provides a separation of concern, which allows you to keep focus on the data modeling and application logic.

Project Setup:
.............

1.create maven project.

2.Add maven dependencies
   <dependencies>
        <dependency>
            <groupId>org.redisson</groupId>
            <artifactId>redisson</artifactId>
            <version>3.17.0</version>
        </dependency>
    </dependencies>
3.Connect Redis Server.
 
Redis server is available in different modes

1.standalone mode
2.cluster mode
3.sentinal mode

1.standalone mode
docker run --name myredisserver --rm -it -p 6379:6379 redis

docker exec -it myredisserver redis-cli
127.0.0.1:6379> keys *
(empty array)
127.0.0.1:6379> set message "Hello"
OK

4.start coding

package com.mycom.redis;

import org.redisson.Redisson;
import org.redisson.api.RKeys;
import org.redisson.api.RedissonClient;
import org.redisson.config.Config;
import redis.clients.jedis.JedisPooled;

public class RedisApp {
    public static void main(String[] args) {
        //connects to default ip address and port
        Config config = new Config();
        config.useSingleServer()
                .setAddress("redis://127.0.0.1:6379");
        RedissonClient client = Redisson.create(config);
        RedissonClient redissonClient = Redisson.create(config);
        RKeys keys = redissonClient.getKeys();
        keys.getKeys().forEach(data -> System.out.println("keys" + data));

    }
}
............................................................................................

How to send key-value to redis server?

package com.mycom.redis.demos;

import org.redisson.Redisson;
import org.redisson.api.RedissonClient;
import org.redisson.config.Config;

public class RedisconnectionUtil {
    static RedissonClient redissonClient;

    public static RedissonClient getClient() {
        Config config = new Config();
        config.useSingleServer()
                .setAddress("redis://127.0.0.1:6379");
        RedissonClient client = Redisson.create(config);
        redissonClient = Redisson.create(config);
        return redissonClient;
    }
}


package com.mycom.redis.demos;

import org.redisson.api.RBucket;
import org.redisson.api.RedissonClient;

public class SetSimpleValue {
    public static void main(String[] args) {
        RedissonClient redissonClient = RedisconnectionUtil.getClient();
        //send key and value .
        //create key
        RBucket<Object> bucket = redissonClient.getBucket("user:name");
        //set value
        bucket.set("subramanian");
        System.out.println( "key : => " + bucket.get());
      //  redissonClient.shutdown();
    }
}
27.0.0.1:6379> get user:1:name
"\x04>\x0bSubramanian"
127.0.0.1:6379>
you can see this output which is not serialized properly.

Data serialization:

	Data serialization is extensively used by Redisson to marshall and unmarshall bytes received or sent over network link with Redis server.
Many popular codecs are available for usage:

if you want to store data in plain string format?
org.redisson.client.codec.StringCodec	String codec


eg:
package com.mycom.redis.demos;

import org.redisson.api.RBucket;
import org.redisson.api.RedissonClient;
import org.redisson.client.codec.StringCodec;

public class SetSimpleValue {
    public static void main(String[] args) {
        RedissonClient redissonClient = RedisconnectionUtil.getClient();
        //send key and value .
        //create key
        RBucket<Object> bucket = redissonClient.getBucket("user:name", StringCodec.INSTANCE);
        //set value
        bucket.set("subramanian");
        System.out.println( "key : => " + bucket.get());
      //  redissonClient.shutdown();
    }
}
...........................................................................................
				How to set TTL 

package com.mycom.redis.demos;

import org.redisson.api.RBucket;
import org.redisson.api.RedissonClient;
import org.redisson.client.codec.StringCodec;

import java.util.concurrent.TimeUnit;

public class TimetoLiveKeys {
    public static void main(String[] args) {
        RedissonClient redissonClient = RedisconnectionUtil.getClient();
        RBucket<String> secretKey = redissonClient.getBucket("user:otp", StringCodec.INSTANCE);
        secretKey.set("3df45",50, TimeUnit.SECONDS);

        //extends
        secretKey.expire(60, TimeUnit.SECONDS);
        long remainTimeToLive = secretKey.remainTimeToLive();
        System.out.println(remainTimeToLive);

    }
}
...................................................................................

How to store Objects?


package com.mycom.redis.demos;

public class Customer {
    private  int id;
    private String name;
    private String city;

    public Customer() {
    }

    @Override
    public String toString() {
        return "Customer{" +
                "id=" + id +
                ", name='" + name + '\'' +
                ", city='" + city + '\'' +
                '}';
    }

    public Customer(int id, String name, String city) {
        this.id = id;
        this.name = name;
        this.city = city;
    }

    public int getId() {
        return id;
    }

    public void setId(int id) {
        this.id = id;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getCity() {
        return city;
    }

    public void setCity(String city) {
        this.city = city;
    }
}


package com.mycom.redis.demos;

import org.redisson.api.RBucket;
import org.redisson.api.RedissonClient;
import org.redisson.codec.TypedJsonJacksonCodec;

public class StoreObject {
    public static void main(String[] args) {
        RedissonClient redissonClient = RedisconnectionUtil.getClient();
        RBucket<Customer> bucket = redissonClient.getBucket("customer:info", new TypedJsonJacksonCodec(Customer.class));
        bucket.set(new Customer(1, "Subramanian", "Coimbatore"));
        Customer customer=bucket.get();
        System.out.println(customer.toString());

    }
}
..............................................................................................
			Send and Receives values using Reactive Extension
			   (Non blocking,Event Driven,Async)


package com.mycom.redis.demos;

import org.redisson.Redisson;
import org.redisson.api.RedissonClient;
import org.redisson.api.RedissonReactiveClient;
import org.redisson.config.Config;

public class RedisconnectionUtil {
    static RedissonClient redissonClient;

    public static RedissonClient getClient() {
        Config config = new Config();
        config.useSingleServer()
                .setAddress("redis://127.0.0.1:6379");
        RedissonClient client = Redisson.create(config);
        redissonClient = Redisson.create(config);
        return redissonClient;
    }
    public static RedissonReactiveClient getReactiveClient(){
        Config config = new Config();
        config.useSingleServer()
                .setAddress("redis://127.0.0.1:6379");
        RedissonClient client = Redisson.create(config);
        redissonClient = Redisson.create(config);
        return redissonClient.reactive();
    }
}
............................................................................................

Reddison:

Reddision communicates with Redis server in three mode

1.sync mode
2.async mode
3.Reactive mode

1.sync mode 
  In this mode commands are sent from app to redis, until response comes , the app is freezed.

RedissonClient client = Redisson.create(config);

RAtomicLong longObject = client.getAtomicLong('myLong');
// sync way
longObject.compareAndSet(3, 401);
set key counter 1000

Redis communication:
   
  Redis client-------commands----------------|
         OK  - Response

2.Async Mode
    In this mode commands are sent from the client to redis, app need not wait until response returned.
   
RFuture<Boolean> result = longObject.compareAndSetAsync(3, 401);

future.whenComplete((res, exception) -> {

    // handle both result and exception

});
// or
future.thenAccept(res -> {

    // handle result

}).exceptionally(exception -> {

    // handle exception

});

Reddision will not have value immediately , rather it will have future Object. you can register listern and you can wait for the result.


Reactive Way:

 Reactive Program is program which models "streams"

 data streams with async model.

 Reactive program offers a features to process data streams in realtime


Redisson exposes Reactive Streams API for most objects and based on two implementations:

1.Reactor

2.Rxjava

RedissonReactiveClient client = redissonClient.reactive();

RAtomicLongReactive atomicLong = client.getAtomicLong("myLong");
Mono<Boolean> cs = longObject.compareAndSet(10, 91);
Mono<Long> get = longObject.get();

get.doOnNext(res -> {
   // ...
}).subscribe();

............................................................................................

Project Reactor api:

1.Mono - emits 0..1  item
2.Flux  - emits 0..N items

   
package com.mycom.redis.demos;

import org.redisson.api.RAtomicLongReactive;
import reactor.core.publisher.Flux;

import java.time.Duration;

public class ReactiveClientUserVisit {
    public static void main(String[] args) {
        //redision
        RAtomicLongReactive atomicLong = RedisconnectionUtil.getReactiveClient()
                .getAtomicLong("user:visit:1");
        //reactor api
        Flux.range(1,50)
                .delayElements(Duration.ofSeconds(1)) //emit numbers after every 1 second
                .flatMap(integer -> atomicLong.incrementAndGet())
                .subscribe(System.out::println);

    }
}
..............................................................................................
				Sync and async
.............................................................................................
Sync work flow.

1.callapi1()-|
2.callapi2()-|
3.....


Async work flow:

1callasyncapi() -----
2.callotherapi()


package com.mycom.redis.demos;

import org.redisson.api.RAtomicLong;
import org.redisson.api.RFuture;

public class AsyncPattern {
    public static void getSync() {
        RAtomicLong longObject = RedisconnectionUtil.getClient().getAtomicLong("user:2:counter");
// sync way
        System.out.println("Start");
        boolean res = longObject.compareAndSet(9000, 9000);
        System.out.println(res);
        System.out.println("end");
    }

    public static void main(String[] args) {
          // getSync();
             getAsync();

    }

    private static void getAsync() {
        RAtomicLong longObject = RedisconnectionUtil.getClient().getAtomicLong("user:2:counter");
// sync way
        System.out.println("Start");
        RFuture<Boolean> booleanRFuture = longObject.compareAndSetAsync(9000, 9000);
        System.out.println("end");

        booleanRFuture.thenAccept(res -> {
            System.out.println("Got Result " + res);
        }).exceptionally(err -> {
            System.out.println("Got Error " + err);
            return null;
        });
    }
}
..............................................................................................
			How to avoid sending multiple keys sequentially - Multi Bucket
...........................................................................................
package com.mycom.redis.demos;

import org.redisson.client.codec.StringCodec;

public class MultiBucket {
    public static void main(String[] args) {
        RedisconnectionUtil.getReactiveClient()
                .getBuckets(StringCodec.INSTANCE)
                .get("user:1:name", "user:2:name", "user:3:name")
                .subscribe(System.out::println);
    }
}
.............................................................................................
				 Event driven Programming with Redis
                                   Redis keyspace notifications
                            Monitor changes to Redis keys and values in real time
.............................................................................................

Keyspace notifications allow clients to subscribe to Pub/Sub channels in order to receive events affecting the Redis data set in some way.

Examples of events that can be received are:

All the commands affecting a given key.
All the keys receiving an LPUSH operation.
All the keys expiring in the database 0.

Note: Redis Pub/Sub is fire and forget that is, if your Pub/Sub client disconnects, and reconnects later, all the events delivered during the time the client was disconnected are lost.

Syntax of event:

PUBLISH __keyspace@0__:mykey del

PUBLISH __keyevent@0__:del mykey

Redis by default will not publish /emit events, we need to configure /activate the events publisher.

Events:
K     Keyspace events, published with __keyspace@<db>__ prefix.
E     Keyevent events, published with __keyevent@<db>__ prefix.
g     Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...
$     String commands
l     List commands
s     Set commands
h     Hash commands
z     Sorted set commands
t     Stream commands
d     Module key type events
x     Expired events (events generated every time a key expires)
e     Evicted events (events generated when a key is evicted for maxmemory)
m     Key miss events (events generated when a key that doesn't exist is accessed)
A     Alias for "g$lshztxed", so that the "AKE" string means all the events except "m".

Two ways :

1.via redis.conf file
2.via cli - through command
   config set notify-keyspace-events AKE


How to listen events inside app?
package com.mycom.redis.demos;

import org.redisson.api.ExpiredObjectListener;
import org.redisson.api.RBucketReactive;
import org.redisson.client.codec.StringCodec;

import java.util.concurrent.TimeUnit;

public class EventListeners {
    public static void main(String[] args) {
        RBucketReactive<String> bucket = RedisconnectionUtil.getReactiveClient().getBucket("user:1:name", StringCodec.INSTANCE);
        //setting value with 10 seconds ttl
        bucket.set("sam", 10, TimeUnit.SECONDS).subscribe();
        //Register listener which listens for redis events
//        bucket.addListener(new ExpiredObjectListener() {
//            @Override
//            public void onExpired(String s) {
//                System.out.println("Expired : " + s);
//            }
//        }).subscribe();
        bucket.addListener((ExpiredObjectListener) s -> System.out.println("Expired : " + s))
                .subscribe();

        try {
            Thread.sleep(11000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }

    }
}
Key Deleted Event:
.................
package com.mycom.redis.demos;

import org.redisson.api.DeletedObjectListener;
import org.redisson.api.RBucketReactive;
import org.redisson.client.codec.StringCodec;
import reactor.core.publisher.Mono;


public class Demo {

    public static void main(String[] args) throws InterruptedException {
        RBucketReactive<String> bucket = RedisconnectionUtil.getReactiveClient().getBucket("user:1:name", StringCodec.INSTANCE);
        Mono<Void> set = bucket.set("sam");
        set.subscribe();

        Mono<Void> get = bucket.get()
                .doOnNext(System.out::println)
                .then();
        get.subscribe();
        Mono<Void> event = bucket.addListener(new DeletedObjectListener() {
            @Override
            public void onDeleted(String name) {
                System.out.println("Deleted : " + name);
            }
        }).then();
        event.block();

    }
}

test :
127.0.0.1:6379> get user:1:name
"sam"
127.0.0.1:6379> del user:1:name
(integer) 1
..............................................................................................
				 Hash -> RedissionMap

package com.mycom.redis.demos;

import org.redisson.api.RMap;
import org.redisson.api.RMapReactive;
import org.redisson.client.codec.StringCodec;
import org.redisson.codec.TypedJsonJacksonCodec;

import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;

public class HashDataStructure {
    public static void main(String[] args) {
//        createReactiveHash();
//        createNormalHash();
//        createHashFromSet();
//        createHashFromJavaMap();
        createHashForObjects();
    }

    private static void createHashForObjects() {
        //how to prisit entity class
        TypedJsonJacksonCodec codec = new TypedJsonJacksonCodec(Integer.class, Student.class);
        RMapReactive<Integer, Student> map = RedisconnectionUtil.getReactiveClient().getMap("users", codec);
        Student student1 = new Student("sam", 10, "atlanta", List.of(1, 2, 3));
        Student student2 = new Student("jake", 30, "miami", List.of(10, 20, 30));

        map.put(1, student1).subscribe();
        map.put(2, student2).subscribe();
    }

    private static void createHashFromJavaMap() {
        RMapReactive<String, String> map = RedisconnectionUtil.getReactiveClient().getMap("user:5", StringCodec.INSTANCE);
        Map<String, String> javaMap = Map.of(
                "name", "jake",
                "age", "30",
                "city", "miami"
        );
        map.putAll(javaMap).then().subscribe();
    }



    private static void createHashFromSet() {
        RMap<String, String> map = RedisconnectionUtil.getClient().getMap("somekey", StringCodec.INSTANCE);
        map.put("1","hello");
        HashSet<String> hashSet = new HashSet<>();
        hashSet.add("1");
        Map<String, String> all = map.getAll(hashSet);
        all.values().forEach(System.out::println);
    }

    private static void createNormalHash() {
        RMap<String, String> map = RedisconnectionUtil.getClient().getMap("user:2:", StringCodec.INSTANCE);
        map.put("name", "subramanian");
        map.put("age", "42");
        map.put("city", "Coimbatore");
        map.values().forEach(System.out::println);

        //update
        map.put("age","40");
        map.values().forEach(System.out::println);

    }

    private static void createReactiveHash() {
        RMapReactive<Object, Object> map = RedisconnectionUtil.getReactiveClient().getMap("user:1", StringCodec.INSTANCE);
        //set values
        map.put("name", "sam").subscribe();
        map.put("age", "10").subscribe();
        map.put("city", "atlanta").subscribe();
        map.get("name").subscribe(System.out::println);
    }
}
.............................................................................................
How to expire single key in map?

package com.mycom.redis.demos;

import org.redisson.api.RMapCacheReactive;
import org.redisson.codec.TypedJsonJacksonCodec;

import java.util.List;
import java.util.concurrent.TimeUnit;

public class HashWithKeyExpire {
    public static void main(String[] args) {
        TypedJsonJacksonCodec codec = new TypedJsonJacksonCodec(Integer.class, Student.class);
        RMapCacheReactive<Integer, Student> mapCache = RedisconnectionUtil.getReactiveClient().getMapCache("users:cache", codec);
        //TTL For Hash keys
        Student student1 = new Student("sam", 10, "atlanta", List.of(1, 2, 3));
        Student student2 = new Student("jake", 30, "miami", List.of(10, 20, 30));

        mapCache.put(1,student1,5, TimeUnit.SECONDS).subscribe();
        mapCache.put(2, student2, 10, TimeUnit.SECONDS).subscribe();

    }
}


package com.mycom.redis.demos;

import org.redisson.api.RMapCacheReactive;
import org.redisson.codec.TypedJsonJacksonCodec;

import java.util.List;
import java.util.concurrent.TimeUnit;

public class HashWithKeyExpire {
    public static void main(String[] args) {
        TypedJsonJacksonCodec codec = new TypedJsonJacksonCodec(Integer.class, Student.class);
        RMapCacheReactive<Integer, Student> mapCache = RedisconnectionUtil.getReactiveClient().getMapCache("users:cache", codec);
        //TTL For Hash keys
        Student student1 = new Student("sam", 10, "atlanta", List.of(1, 2, 3));
        Student student2 = new Student("jake", 30, "miami", List.of(10, 20, 30));

        mapCache.put(1,student1,5, TimeUnit.SECONDS).subscribe();
        mapCache.put(2, student2, 10, TimeUnit.SECONDS).subscribe();

    }
}
............................................................................................
				 Building Cache Infrastructure
					using MapCache
...........................................................................................
					

package com.mycom.redis.demos;

import org.redisson.api.LocalCachedMapOptions;
import org.redisson.api.RLocalCachedMap;
import org.redisson.api.RedissonClient;
import org.redisson.codec.TypedJsonJacksonCodec;
import reactor.core.publisher.Flux;

import java.time.Duration;
import java.util.List;

public class ServiceOne {
    public static void main(String[] args) {
        RLocalCachedMap<Integer, Student> studentsMap;
        RedissonClient redissonClient = RedisconnectionUtil.getClient();
        //LocalCache Configuration
        LocalCachedMapOptions<Integer, Student> mapOptions = LocalCachedMapOptions
                .<Integer, Student>defaults()
                .syncStrategy(LocalCachedMapOptions.SyncStrategy.UPDATE)
                .reconnectionStrategy(LocalCachedMapOptions.ReconnectionStrategy.NONE);

        studentsMap = redissonClient.getLocalCachedMap(
                "students",
                new TypedJsonJacksonCodec(Integer.class, Student.class),
                mapOptions
        );
        Student student1 = new Student("sam", 10, "atlanta", List.of(1, 2, 3));
        Student student2 = new Student("jake", 30, "miami", List.of(10, 20, 30));
        studentsMap.put(1, student1);
        studentsMap.put(2, student2);

        Flux.interval(Duration.ofSeconds(1))
                .doOnNext(i -> System.out.println(i + " ==> " + studentsMap.get(1)))
                .blockLast();

        //sleep(600000);
    }

}

package com.mycom.redis.demos;
import org.redisson.api.LocalCachedMapOptions;
import org.redisson.api.RLocalCachedMap;
import org.redisson.api.RedissonClient;
import org.redisson.codec.TypedJsonJacksonCodec;

import java.util.List;

public class ServiceTwo {
    public static void main(String[] args) {
        RLocalCachedMap<Integer, Student> studentsMap;
        RedissonClient redissonClient = RedisconnectionUtil.getClient();

        LocalCachedMapOptions<Integer, Student> mapOptions = LocalCachedMapOptions.<Integer, Student>defaults()
                .syncStrategy(LocalCachedMapOptions.SyncStrategy.UPDATE)
                .reconnectionStrategy(LocalCachedMapOptions.ReconnectionStrategy.NONE);

        studentsMap = redissonClient.getLocalCachedMap(
                "students",
                new TypedJsonJacksonCodec(Integer.class, Student.class),
                mapOptions
        );
        Student student1 = new Student("sam", 10, "atlanta", List.of(1, 2, 3));
        //something change
        student1.setName("Subramanian M");
        studentsMap.put(1, student1);
    }
}
.............................................................................................
					List
..............................................................................................

List is LinkedList in Redis.
List can be used to implement "Stack,Queue" like datastructure.					

package com.mycom.redis.demos;

import org.redisson.api.RListReactive;
import org.redisson.client.codec.LongCodec;

import java.util.List;
import java.util.stream.Collectors;
import java.util.stream.LongStream;

public class ListImplementation {
    public static void main(String[] args) {
        RListReactive<Object> list = RedisconnectionUtil.getReactiveClient().getList("numbers-input", LongCodec.INSTANCE);
        //push data.
        List<Long> longList = LongStream.rangeClosed(1, 20)
                .boxed()
                .collect(Collectors.toList());
         list.addAll(longList).subscribe();

    }
}

127.0.0.1:6379> lrange numbers-input 0 -1
 1) "1"
 2) "2"
 3) "3"
 4) "4"
 5) "5"
 6) "6"
 7) "7"
 8) "8"
 9) "9"
10) "10"
11) "11"
12) "12"
13) "13"
14) "14"
15) "15"
16) "16"
17) "17"
18) "18"
19) "19"
20) "20"
.............................................................................................

How to implement Queue DataStructure?

package com.mycom.redis.demos;

import org.redisson.api.RQueueReactive;
import org.redisson.client.codec.LongCodec;

public class QueueImplmentation {
    public static void main(String[] args) {
        RQueueReactive<Object> queue = RedisconnectionUtil.getReactiveClient().getQueue("numbers-input", LongCodec.INSTANCE);
        queue.poll().repeat(3).subscribe();

    }
}

127.0.0.1:6379> lrange numbers-input 0 -1
 1) "5"
 2) "6"
 3) "7"
 4) "8"
 5) "9"
 6) "10"
 7) "11"
 8) "12"
 9) "13"
10) "14"
11) "15"
12) "16"
13) "17"
14) "18"
15) "19"
16) "20"
..............................................................................................

Stack: 
in order to create Stack, we use double ended,dqueue

package com.mycom.redis.demos;

import org.redisson.api.RDequeReactive;
import org.redisson.client.codec.LongCodec;

public class StackDemo {
    public static void main(String[] args) {
        RDequeReactive<Object> stack = RedisconnectionUtil.getReactiveClient().getDeque("numbers-input", LongCodec.INSTANCE);
        stack.pollLast().repeat(3).subscribe();
    }
}

127.0.0.1:6379> lrange numbers-input 0 -1
1) "9"
2) "10"
3) "11"
4) "12"
5) "13"
6) "14"
7) "15"
8) "16"
............................................................................................
			How to build Message Queue using Redis

in order to create message queue there are two actors

1.publisher
2.subscriber


package com.mycom.redis.demos;

import org.redisson.api.RBlockingDequeReactive;
import org.redisson.client.codec.LongCodec;
import reactor.core.publisher.Flux;

import java.time.Duration;

public class Publisher {
    public static void main(String[] args) {
        RBlockingDequeReactive<Object> msgQueue = RedisconnectionUtil.getReactiveClient().getBlockingDeque("message-queue", LongCodec.INSTANCE);
        Flux.range(1, 100)
                .delayElements(Duration.ofMillis(500))
                .doOnNext(i -> System.out.println("going to add " + i))
                .flatMap(i -> msgQueue.add(Long.valueOf(i)))
                .subscribe();
        sleep(600_000);
    }

    private static void sleep(int i) {
        try {
            Thread.sleep(i);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }
}
package com.mycom.redis.demos;
import org.redisson.api.RBlockingDequeReactive;
import org.redisson.client.codec.LongCodec;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;

public class Consumer {
    public static void main(String[] args) {
        RBlockingDequeReactive<Long> msgQueue = RedisconnectionUtil.getReactiveClient().getBlockingDeque("message-queue", LongCodec.INSTANCE);

        msgQueue.takeElements()
                .doOnNext(i -> System.out.println("Consumer 1 : " + i))
                .doOnError(System.out::println)
                .subscribe();
        sleep(600_000);
    }
    private static void sleep(int i) {
        try {
            Thread.sleep(i);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }
}
...........................................................................................

					HyperLogLog

Computing the count of distinct elements in massive data sets is often necessary but computationally intensive. Say you need to determine the number of distinct people visiting Facebook in the past week using a single machine. Doing this with a traditional SQL query on a data set as massive as the ones we use at Facebook would take days and terabytes of memory. To speed up these queries, we implemented an algorithm called HyperLogLog (HLL) in Presto, a distributed SQL query engine. HLL works by providing an approximate count of distinct elements using a function called APPROX_DISTINCT. With HLL, we can perform the same calculation in 12 hours with less than 1 MB of memory. We have seen great improvements, with some queries being run within minutes, including those used to analyze thousands of A/B tests.

package com.mycom.redis.demos;

import org.redisson.api.RHyperLogLogReactive;
import org.redisson.client.codec.LongCodec;
import reactor.core.publisher.Flux;
import java.util.List;
import java.util.stream.Collectors;
import java.util.stream.LongStream;

public class HyperLogLogDemo {
    public static void main(String[] args) {
        RHyperLogLogReactive<Long> counter = RedisconnectionUtil.getReactiveClient().getHyperLogLog("user:visits", LongCodec.INSTANCE);
        List<Long> list1 = LongStream.rangeClosed(1, 25000)
                .boxed()
                .collect(Collectors.toList());

        List<Long> list2 = LongStream.rangeClosed(25001, 50000)
                .boxed()
                .collect(Collectors.toList());

        List<Long> list3 = LongStream.rangeClosed(1, 75000)
                .boxed()
                .collect(Collectors.toList());

        List<Long> list4 = LongStream.rangeClosed(50000, 100_000)
                .boxed()
                .collect(Collectors.toList());

        Flux.just(list1, list2, list3, list4)
                .flatMap(counter::addAll)
                .then().subscribe();

        System.out.println("Approxmiate Counters");
        counter.count()
                .doOnNext(System.out::println)
                .subscribe();
    }
}
..............................................................................................
					Pub-Sub Pattern

Redis Pub/Sub
How to use pub/sub channels in Redis

SUBSCRIBE, UNSUBSCRIBE and PUBLISH implement the Publish/Subscribe messaging paradigm 

package com.mycom.redis.demos;

import org.redisson.api.RTopicReactive;
import org.redisson.client.codec.StringCodec;

public class PubSubDemo {
    public static void main(String[] args) {
        //consumer code
        RTopicReactive topic = RedisconnectionUtil.getReactiveClient().getTopic("slack-room1", StringCodec.INSTANCE);
        topic.getMessages(String.class)
                .doOnError(System.out::println)
                .doOnNext(System.out::println)
                .subscribe();

    }

    private static void sleep(int i) {
        try {
            Thread.sleep(i);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }
}
27.0.0.1:6379> publish slack-room1 "Hello,Today is Holiday"
(integer) 1
127.0.0.1:6379> publish slack-room1 "Hello,Today is Holiday"
(integer) 2
12

Pattern based message delivery:
what if one person wants to listen the message from many slack rooms.

package com.mycom.redis.demos;

import org.redisson.api.RPatternTopicReactive;
import org.redisson.api.listener.PatternMessageListener;
import org.redisson.client.codec.StringCodec;

public class PatternBasedPubSubListener {
    public static void main(String[] args) {
        RPatternTopicReactive patternTopic = RedisconnectionUtil.getReactiveClient().getPatternTopic("news*", StringCodec.INSTANCE);
        patternTopic.addListener(String.class, new PatternMessageListener<String>() {
            @Override
            public void onMessage(CharSequence pattern, CharSequence topic, String msg) {
                System.out.println(pattern + " : " + topic + " : " + msg);
            }
        }).subscribe();
        sleep(600_000);
    }

    private static void sleep(int i) {
        try {
            Thread.sleep(i);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }
}
..............................................................................................
					Redis-PipeLine
..............................................................................................

How to optimize round-trip times by batching Redis commands
   Redis pipelining is a technique for improving performance by issuing multiple commands at once without waiting for the response to each individual command. Pipelining is supported by most Redis clients. 

package com.mycom.redis.demos;

import org.redisson.api.BatchOptions;
import org.redisson.api.RBatchReactive;
import org.redisson.api.RListReactive;
import org.redisson.api.RSetReactive;
import org.redisson.client.codec.LongCodec;

public class RedisPipeLine {
    public static void main(String[] args) {
        RBatchReactive batch = RedisconnectionUtil.getReactiveClient().createBatch(BatchOptions.defaults());
        RListReactive<Long> list = batch.getList("numbers-list", LongCodec.INSTANCE);
        RSetReactive<Long> set = batch.getSet("numbers-set", LongCodec.INSTANCE);

        for (long i = 0; i < 500_000; i++) {
            list.add(i);
            set.add(i);
        }
        batch.execute().doOnTerminate(()->{
            System.out.println("done");
        }).subscribe(System.out::println);
    }
}
............................................................................................
				Redis-Transactions

Transactions
	Redis Transactions allow the execution of a group of commands in a single step, they are centered around the commands MULTI, EXEC, DISCARD and WATCH. Redis Transactions make two important guarantees:


package com.mycom.redis.demos;

import org.redisson.api.RBucket;
import org.redisson.api.RMap;
import org.redisson.api.RTransaction;
import org.redisson.api.TransactionOptions;

public class Transactions {
    public static void main(String[] args) {
        RBucket<String> redisson = RedisconnectionUtil.getClient().getBucket("test");
        redisson.set("123");
        RTransaction transaction = RedisconnectionUtil.getClient().createTransaction(TransactionOptions.defaults());
        //grouping two differnt operation as single execution-transaction
        RBucket<String> bucket = transaction.getBucket("test");
        bucket.set("234");
        RMap<String, String> map = transaction.getMap("myMap");
        map.put("1", "2");
        transaction.commit();
    }
}
..............................................................................................
				 Spring and Redis Integration
.............................................................................................

1.Spring Data Redis
   Abstraction on Jedis and Lettuce

Features:
Connection package as low-level abstraction across multiple Redis drivers(Lettuce and Jedis).

Exception translation to Spring’s portable Data Access exception hierarchy for Redis driver exceptions.

RedisTemplate that provides a high-level abstraction for performing various Redis operations, exception translation and serialization support.

Pubsub support (such as a MessageListenerContainer for message-driven POJOs).

Redis Sentinel and Redis Cluster support.

Reactive API using the Lettuce driver.

JDK, String, JSON and Spring Object/XML mapping serializers.

JDK Collection implementations on top of Redis.

Atomic counter support classes.

Sorting and Pipelining functionality.

Dedicated support for SORT, SORT/GET pattern and returned bulk values.

Redis implementation for Spring 3.1 cache abstraction.

Automatic implementation of Repository interfaces including support for custom query methods using @EnableRedisRepositories.

2.Reddission
   -OpenSource
   -Commericial

Spring Boot Integration with Reddision.

Step 1:

start.spring.io

 create Reactive Web Project

Step 2:
	<dependency>
			<groupId>org.redisson</groupId>
			<artifactId>redisson-spring-boot-starter</artifactId>
			<version>3.17.0</version>
		</dependency>

3.Create Controller , start Redission;

package com.example.demo;

import org.redisson.api.RBucket;
import org.redisson.api.RBucketReactive;
import org.redisson.api.RedissonClient;
import org.redisson.client.codec.StringCodec;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;
import reactor.core.publisher.Mono;

@RestController
public class RedisController {

    @Autowired
    private RedissonClient redissonClient;

    @GetMapping("/hello")
    public String getMessage() {
        RBucket<String> message = redissonClient.getBucket("message", StringCodec.INSTANCE);
        return message.get();
    }

    @GetMapping("/helloreactive")
    public Mono<String> getMessageReactive() {
        RBucketReactive<String> message = redissonClient.reactive().getBucket("message", StringCodec.INSTANCE);
        return message.get();

    }
}
..............................................................................................
				Caching Using Spring Boot and Redis
.............................................................................................. 

What is cache / Caching?

 In computing, a cache is a component that transparently stores data so that future requests
for that data can be served faster.

The data that is stored within a cache might be values that have been computed earlier or duplicates of original values that are stored elsewhere.


Before this, you need to understand one concept, called "IO".


IO - read and write.

Write/save/insert:
  moving data into some place

Place:
 - In memory
 - Disk
 - Network Socket
 - Processors
 - External Storage devices.

IN Memory: Random access memory :RAM.
......................................
- Writing data into RAM.

Which is faster, but not durable.

Disk:
  -Writing data into hard disk.
Eg:
  file systems
     -Databases-RDBMS
Which is slow but durable.

Network Socket:
 -socket is entry and exit point of networks
 -apps write data into socket via os kernal, which intern transfer data to other machines 
  in the network.

-Processors
   you can store data inside cpu registers for faster access.

- External Storage devices
   -physical storage like pendrives,extrnal harddisk....
   -Cloud Storage.

Reading:
 -Reading requires more responsive- end user should able to get data very very faster.

Reads costs more when we talk to disk(file system,databases,remote storages) based data.

How to improve read performance?
................................

if any application / users reads the same data again and again, dont hit disk every time, rather make snapeshot of that data in first hit, keeps that "IN Memory(RAM) / CPU Register"
so that future requests served very faster : Caching

Real world examples:

 if application sends an sql query request to database engine.

-The database engine 
   -parses the query
   -Query Excution plan
   -Query will be compiled - binary image of that query
   -Query execution - Database engine will do sys call to disk
   -Read operation begins
   -Results are prepared
   -Send back to Client Application

imagine, if application repeats the same process again.

How to improve Network reads : WEB

If web clients ask some web documents such as html,pdf,image,json,xml to the webserver.

-webserver will do low level io calls-read

if web clients ask the same document again and again, we need to improve performance, so that 
HTTP protocals having feature called storing repeated content some where(IN Memory)-HTTP caching.

Hardwares :How to avoid reading data within hardware devices

-Hardware cache
A CPU cache is a hardware cache used by the central processing unit (CPU) of a computer to reduce the average cost (time or energy) to access data from the main memory. A cache is a smaller, faster memory, located closer to a processor core, which stores copies of the data from frequently used main memory locations.
.............................................................................................
	 "Caching is nothing but how to improve IO( Frequrent READ of same data) operation"

Types of Caching:
................
Based on implementations

-hardware level caching
-software level caching
   -Disk cache -databases,filesystems....
   -Web cache - webservers,proxy servers, cdn
   -Memoization - Program level caching, to avoid code repetation cycles(loop)
                  - languages -pythyon,groovy,javascript-most of the functional pl
   -Application level caching with caching components
         -Caching in Java Apps.
         -Spring framework abstracts away caching soultions in enterprise/micro services
          application.

-network level caching
   -protocal based caching-TCP
   -HTTP
   -SMTP.
............................................................................................
How to implement caching: Cache algorthims/Policy

caching is all about io.

-write 
-read
-Replacement / Eviction.

Write Policies:
.............
 It defines how to interact with cache component/architecture, when application starts writing data.

1.Write Through
2.Write Around
3.Write Back
4.Cache aside pattern

1.Write through pattern/policy:
...............................

1.When ever write request comes , write operation will be done in cache system.
2.From the Cache , data will be written to Database System.
3.Once Data is written in database successfully, acknowledgement will be sent to Client Application.

4.Application has to wait until the response come from cache(not suitable for blocking apps)
  (may be suitable for async /non blocking application).
5.It increases latency, since cache interacts with db,db interacts with cache and app.

2.Write around policy:
.......................

1.Applications write data to database directly

How read works?

2.When read requests, Application hits Cache
3.If no data found in the cache first time, Cache loads data from Database into caching
  system
4.If data found in the cache , data will be returned from cache itself.

3.Write around policy:
.......................

1.Applications write data to database directly
2.When read requests, Application hits Cache
3.If no data found in the cache first time, Cache loads data from Database into caching
  system
4.If data found in the cache , data will be returned from cache itself.


3.Write Back Policy:
...................

1.Application sends write requests to Cache System, once write is completed in Cache, Acknowledgement sent to Client Application.

There is background service, which starts writing data from Cache into Database Async


4.Cache aside pattern

1.Application hits cache system
2.if data is present in the cache , the data will be reterived from the Cache itself
3.if no data is present in the cache , the application will query the data from the database.
4.Application will update the data in the cache system.


                       "Cache Aside Pattern is mostly Used Pattern in apps"

Use cases : Advantages and Disadvantages

This pattern is more general purpose pattern, best for  "read-heavy work loads"

Tecnologies used :

1.Memcached
2.Redis
.............................................................................................
				 Read Policy
.............................................................................................

1.Cache Hit
2.Cache Miss

Cache Hit:
..........
Cache hit means the requested data already there in the cache.
This request can be served by simply reading the cache, which is comparatively faster
Hit happens , subsquent calls only, first time it wont happen.

Cache Miss:
...........

1.The data has to be recomputed or fetched from its original storage location(database/file syste/app process)

2.A cache miss occurs either
   2.1.Because the data was never placed in the cache,
   2.2 Because the data was removed (“evicted”) from the cache by either the caching system itself or an external application that specifically made that eviction request.
............................................................................................
			Cache Replacement-Eviction Policy - Memory Management
............................................................................................

This is policy , implemented by cache providers in order to manage memory.

How to clear stale /unwanted /un used data from the cache system?
  This process of removing data from cache system is called "eviction".

How to evict?

-manual evication
-automatic evication.
  Systems like redis offers an alogorthims-TTL - Time to live.(how long data can be inside
  System, once timeout, data will be removed automatically.


In order to evit data from cache system, there are plent of algorthims.

1.Bélády's algorithm
2.First in first out (FIFO)
3.Last in first out (LIFO) or First in last out (FILO)
4.Least recently used (LRU)
5.Time aware least recently used (TLRU)
6.Most recently used (MRU)
8.Pseudo-LRU (PLRU)
9.Random replacement (RR)
10.Segmented LRU (SLRU)
12.Least-frequently used (LFU)
13.Least frequent recently used (LFRU)
14.LFU with dynamic aging (LFUDA)
15.Low inter-reference recency set (LIRS)
16.CLOCK-Pro
17.Adaptive replacement cache (ARC)
18.AdaptiveClimb (AC)
19.Clock with adaptive replacement (CAR)
20.Multi queue (MQ)
21.Pannier: Container-based caching algorithm for compound objects

Redis  and Key eviction Policy:

Least recently used (LRU):
   Redis uses this alorthim.

https://redis.io/docs/manual/config/#configuring-redis-as-a-cache

Redis uses redis.conf file where we can configure the key removal policy.

maxmemory 2mb
maxmemory-policy allkeys-lru

 "Here , we have used maxmemory 2mb" , redis can store keys upto 2mb only
  what if redis reaches 2mb memory, it will start removing keys according to the key eviction
 policy selected.
   Here we have selected "allkeys-lru" -  delete all recently used(read) keys
 
If Redis can't remove keys according to the policy, or if the policy is set to 'noeviction', Redis will start to reply with errors to commands that would use more memory, like SET, LPUSH, and so on, and will continue  to reply to read-only commands like GET.


"In this configuration there is no need for the application to set a time to live for keys using the EXPIRE command (or equivalent) since all the keys will be evicted using an approximated LRU algorithm as long as we hit the 2 megabyte memory limit."
..............................................................................................
			Application Caching  Soultions and Implementation
.............................................................................................

Application caching provides, how to store data inside application.

Application caching can be classified into two types:
.....................................................

1.Local Cache
2.Distributed Cache

Both cache implementation will store data in side RAM Only.

1.Local Cache : (Cache System) 


1.1. JCache Spec:
 
 Cache is provided iniside application as "Data Structure".
 if you are working in java application , java provides a spec called jcache
JSR 107: JCACHE - Java Temporary Caching API

 "JCache is a de facto standard Java cache API for caching data. Also known as JSR 107 (i.e., a “Java Specification Request” from the “Java Community Process” [JCP]), this API implementation is intended to create a way for different technologies to provide a common caching interface. It defines the mechanism for creating, accessing, updating, and removing information from a cache."


1.2.Spring

 IF you  are working in spring framework, Spring framework provides an abstraction

  "Caching Abstraction" : set of apis and annotations , developers start building caching system without worring about underlaying cache implemenation(systems-like memcache,redis,hazlecast....)

https://docs.spring.io/spring-framework/docs/3.2.x/spring-framework-reference/html/cache.html


          "Cache data is maintained inside Application Process(JVM)"

Spring provides an data structure called "ConcurrentHashMapCache" 
  -it is map datastructure
  - used to store data inside app.

Note:

  In any caching system, data is stored in side data structure only : KEY-VALUE pair datastructure : Dictionary/HashMap/Map
   This is fundamental storage model of any caching system.
..............................................................................................
				Distributed Cache  - Data Grid
.............................................................................................
                 Cache data is maintained outside application process(JVM).


Types of Cache:

1.Local Cache
   Cache is implemented at application level using ConcurrentHashMap or Reddision like implementation.
2.Data Grid
   Distributed Cache like redis, memcached
3.JPA /Hibernate First Level Cache and Second Level Cache
4.Hybrid

Place of Cache
1.Database
2.Heap,
3.HTTP Proxy
4.Browser
5.Prozessor, Disk, Off Heap, Persistence-Framework, Application

local and distributed caching at the application level with the Spring Framework

Where shall I cache?
Identify suitable layers for caching
  -Controller layer - HTTP Caching
  -Biz layer /integration layer - read operations -  Local/Distributed cache
  -Repository layer -  JPA/Hibernate First/second level caching

Place of Cache:
 Local Cache - In side Memory(RAM)- Heap Memory.

 Clustered(Distributed) In Memory Cached
 
..............................................................................................

Distributed Cache:

 Keeps data outside JVM - CacheNode(redis,memcached)
.............................................................................................
Local vs Distributed

Local Cache:
 -Part of application
 -easy to implement
 -very fast, no lantency.
-as application shuts down, no where data is persisted.
-it is diffcult in concurrency
-data cant be replicated
-heap size will increase , if you store more data.
-Avoid big heaps just for caching
-Big heap leads to long major GCs


Distributed Cache:
 -outside application process
 -it is easy to scale across clusters of nodes.
 -it is easy to make highly  available.
 -It is persisted-you can save data across time.
etc.......

Use a distributed cache for big amounts of data

Distributed caches implemented cache nodes.

Cache Nodes:

1.Hazelcast
2.Redis
3.memcached
4.Appache zoo keeper
5.Generic.
6.EhCache 2. x.
7.Infinispan
..............................................................................................
  	Application Caching Implementation using Spring Boot caching Abstraction
...........................................................................................

1.Spring boot can support local and distributed cache providers
2.Spring boot offers abstraction apis to work with any cache providers without changing app
  code, you can switch any provider any time.
3.Spring boot uses cache starter packages.



Spring Application Setup and how to employ caching.
...................................................

Application Req:
 -It could be simple Databse CURD app
 -It could be complex microservice application.

App settings:

Lab Guide:

1.go to 
https://start.spring.io/

2.Add springweb,springdata,springcache dependencies

pom.xml
<dependency>
	<groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-cache</artifactId>
</dependency>

Provides Cache Annotations

How to enable caching in spring Application?

@EnableCaching:

  It tries to find out any cache provider in the class path, if not it fallback to inmemory cache provider

eg:
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cache.annotation.EnableCaching;

@SpringBootApplication
@EnableCaching
public class DemoApplication {

	public static void main(String[] args) {
		SpringApplication.run(DemoApplication.class, args);
	}

}

Step 2: Cache Provider -  Redis

add Reddison Depedency in pom.xml
   <dependency>
            <groupId>org.redisson</groupId>
            <artifactId>redisson-spring-boot-starter</artifactId>
            <version>3.17.0</version>
        </dependency>

package com.example.demo.config;

import org.redisson.api.RedissonClient;
import org.redisson.spring.cache.RedissonSpringCacheManager;
//import org.springframework.cache.CacheManager;
import org.springframework.cache.CacheManager;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class ReddisonConfig {
    @Bean
    public CacheManager cacheManager(RedissonClient reddisonClient) {
        return new RedissonSpringCacheManager(reddisonClient);
    }
}

Step 3: Annotate which method need to mark as cachable
package com.example.demo.greeter.cache;

import org.springframework.cache.annotation.Cacheable;
import org.springframework.stereotype.Service;


@Service
public class GreeterService {

    @Cacheable("greet:message")
    public String sayHello() {
        System.out.println("Say Hello is called");
        return "Hello";
    }
}

Step 4: Test with Redis
27.0.0.1:6379> hgetall  greet:message
1) "\x04\x04\t>/org.springframework.cache.interceptor.SimpleKey\x848\xff\xdb\xd2\xe7\xe3p\x00\x00\x00\x01>\x06params\x16\x00\x16A\x16"
2) "\x04>\x05Hello"
127.0.0.1:6379>

How and where caching features are applied.
...........................................

 Caching features are added on top of biz apis - find,update,delete


At its core, the cache abstraction applies caching to Java methods, thus reducing the number of executions based on the information available in the cache.

That is, each time a targeted method is invoked, the abstraction applies a caching behavior that checks whether the method has been already executed for the given arguments.

If it has been executed, the cached result is returned without having to execute the actual method.

 If the method has not been executed, then it is executed, and the result is cached and returned to the user so that, the next time the method is invoked the cached result is returned.

This way, expensive methods (whether CPU- or IO-bound) can be executed only once for a given set of parameters and the result reused without having to actually execute the method again

The caching logic is applied transparently without any interference to the invoker.

"This approach works only for methods that are guaranteed to return the same output (result) for a given input (or arguments) no matter how many times it is executed."

Annotations overview:
.....................

@Cachable:
As the name implies, you can use @Cacheable to demarcate methods that are cacheable 
-that is, methods for which the result is stored in the cache so that, on subsequent invocations (with the same arguments), the value in the cache is returned without having to actually execute the method.

parameterize it with the name of the cache where the results would be stored.

@Cacheable("books")
public Book findBook(ISBN isbn) {...}

@Cacheable("books")
public Book findBook(ISBN isbn) {...}

The findBook() call will first check the cache books before actually invoking the method and then caching the result.

-In the preceding snippet, the findBook method is associated with the cache named books.

-Each time the method is called, the cache is checked to see whether the invocation has already been executed and does not have to be repeated.

-While in most cases, only one cache is declared, the annotation lets multiple names be specified so that more than one cache is being use.

-. In this case, each of the caches is checked before executing the method — if at least one cache is hit, the associated value is returned.
..............................................................................................

@Cachable Annotation with Key attribute:
.........................................
 
Controller:

@GetMapping("/{index}/{name}")
    public Mono<Integer> getFib(@PathVariable int index, @PathVariable String name) {
        return Mono.fromSupplier(() -> this.service.getFib(index, name));
    }
 @Cacheable(value = "math:fib")
    public int getFib(int index, String name) {
        System.out.println("calculating fib for " + index + " " + name);
        return this.fib(index);
    }

now if you hit end point

http://localhost:8080/fib/50/bar 
  response would be -298632863

http://localhost:8080/fib/50/foo

response would be  -298632863

when you look at logs in the console

calculating fib for 50 bar

calculating fib for 50 foo

Here method is called two times for foo and bar but fib result is same for both use case.

Cache value is calculated based on no of parameters on the methods where @cachable annotation is declared

    @Cacheable(value = "math:fib")
    public int getFib(int index, String name) {
        System.out.println("calculating fib for " + index + " " + name);
        return this.fib(index);
    }

here if index and name is same for subsquent request , the output will be from the cache only.
if name changes it will recompute and the result will be returned.

  "I want to cache only based on index parameter only not name"

 Solution is key attribute

   @Cacheable(value = "math:fib", key = "#index")
    public int getFib(int index, String name) {
        System.out.println("calculating fib for " + index + " " + name);
        return this.fib(index);
    }

  @Cacheable(value = "product", key = "#product.id")
    public int getProduct(Product product, String name) {
        System.out.println("calculating fib for " + index + " " + name);
        return this.fib(index);
    }

Default key value  is "", meaning all method parameters are considered as a key, unless a custom keyGenerator() has been configured.

KeyGenerator:

-This is responsible for generating every key for each data item in the cache, which would be used to lookup the data item on retrieval.
-The default implementation here is the SimpleKeyGenerator – which uses the method parameters provided to generate a key

The default key generator

By default, SimpleKeyGenerator in the org.springframework.cache.interceptor package, an implementation of KeyGenerator interface, is used to generate the cache key

SimpleKeyGenerator evaluates parameters of the cache annotated methods (by @Cachable, @CachePut and @CacheEvict). If only one non-null param is existing, it returns the param itself, otherwise the below SimpleKey's toString() method is used for computing all params

@Override
public String toString() {  
    return getClass().getSimpleName() + " [" + StringUtils.arrayToCommaDelimitedString(this.params) + "]";
}

Default Key Generation

The caching abstraction uses a simple KeyGenerator based on the following algorithm.

-If no params are given, return SimpleKey.EMPTY.

-If only one param is given, return that instance.

-If more than one param is given, return a SimpleKey that contains all parameters


Default Key Generation code

public class CustomKeyGenerator implements KeyGenerator {

 @Override
 public Object generate(Object target, Method method, Object...params) {
  return generateKey(params);
 }

 /**
  * Generate a key based on the specified parameters.
  */
 public static Object generateKey(Object...params) {
  if (params.length == 0) {
   return CustomCacheKey.EMPTY;
  }
  if (params.length == 1) {
   Object param = params[0];
   if (param != null && !param.getClass().isArray()) {
    return param;
   }
  }
  return new CustomCacheKey(params);
 }
}
/////////////////////////////////////////////////////////////////////////////////////////////

IS it Recommended to use default key generation ?

No!
Because it may cause unexpected key collisions.

Recommended use keys parameters:

The @Cacheable annotation lets you specify how the key
 is generated through its key attribute

@Cacheable(cacheNames="books", key="#isbn")
public Book findBook(ISBN isbn, boolean checkWarehouse, boolean includeUsed)

# - spring expression language syntax.
isbn - key reference. here we are using entire object as key.

Note:
ISBN isbn and key="#isbn" should match.

@Cacheable(cacheNames="books", key="#isbn.rawNumber")
public Book findBook(ISBN isbn, boolean checkWarehouse, boolean includeUsed)

Here we are taking isbn.rawNumber which is one of the field value for cache.
.............................................................................................
				conditional Caching
.............................................................................................

I dont want to cache every thing, i need to cache only some values based on some condition.

eg:
 I want to cache products only which are available(inStock).

@Cacheable("book", condition="#book.isStockAvailable") 
public Book findBook(Book book){

}

  @Cacheable(value = "math:fib", key = "#index" ,condition = "#index==5")
    public int getFib(int index, String name) {
        System.out.println("calculating fib for " + index + " " + name);
        return this.fib(index);
    }

http://localhost:8080/fib/5/bar
calculating fib for 5 bar

http://localhost:8080/fib/10/bar
calculating fib for 10 bar
calculating fib for 10 bar
calculating fib for 10 bar
calculating fib for 10 bar


unless:
.......
We can also control the caching based on the output of the method rather than the input – via the unless parameter

condition vs unless 
  both are same.

condition is applied before method execution : input parameters
unless is appled after method execution : output parameter


   @Override
    @Cacheable(cacheNames = "books",  unless = "#id==3")
    public Book getBook(long id) {
        logger.info("fetching book from db" + id);
        Optional<Book> book = bookRepository.findById(id);
        if (book.isPresent()) {
            return book.get();
        } else {
            return new Book();
        }
    }

Here , we tell cache everything but not id 3.
.............................................................................................
				Multi threading and caching:

 if  a  method is by multiple thread of execution , there might be inconsistency in data reterival.


Synchronized caching

In a multi-threaded environment, certain operations might be concurrently invoked for the same argument (typically on startup). By default, the cache abstraction does not lock anything and the same value may be computed several times, defeating the purpose of caching.

For those particular cases, the sync attribute can be used to instruct the underlying cache provider to lock the cache entry while the value is being computed. As a result, only one thread will be busy computing the value while the others are blocked until the entry is updated in the cache.

@Cacheable(cacheNames="foos", sync=true)
public Foo executeExpensiveOperation(String id) {...}

This is an optional feature and your favorite cache library may not support it. All CacheManager implementations provided by the core framework support it. Check the documentation of your cache provider for more details.

..............................................................................................
				Cache Eviction
..............................................................................................

How to evict cache keys?

There are two ways

1.In the cache provider side(redis side)
   By setting properties 
  max-memory : 2mb
  cache-eviction-policy: all-lrukeys
  
2.In the applications side

 -> By manual process
 -> By automation using schedulers

 By manual process:

  @CacheEvict(value = "math:fib", key = "#index")
    public void clearCache(int index) {
        System.out.println("clearing hash key for " + index);
    }
  @GetMapping("{index}/clear")
    public Mono<Void> clearCache(@PathVariable int index){
        return Mono.fromRunnable(() -> this.service.clearCache(index));
    }

http://localhost:8080/fib/5/clear

127.0.0.1:6379> keys *
1) "math:fib"
2) "weather"

127.0.0.1:6379> keys *
1) "weather"
1

By automation using schedulers:

    @Scheduled(fixedRate = 10_000)
    @CacheEvict(value = "math:fib", allEntries = true)
    public void clearCache() {
        System.out.println("clearing all fib keys");
    }

@SpringBootApplication
@EnableCaching
@EnableScheduling
public class DemoApplication {

	public static void main(String[] args) {
		SpringApplication.run(DemoApplication.class, args);
	}
}
.............................................................................................
				How to update Cache
.............................................................................................

- "When the cache needs to be updated without interfering with the method execution, you can use the @CachePut annotation.

 It supports the same options as @Cacheable and should be used for cache population rather than method flow optimization. The following example uses the @CachePut annotation:

"	Using @CachePut and @Cacheable annotations on the same method is generally strongly discouraged because they have different behaviors. While the latter causes the method execution to be skipped by using the cache, the former forces the execution in order to execute a cache update. This leads to unexpected behavior and, with the exception of specific corner-cases (such as annotations having conditions that exclude them from each other), such declarations should be avoided. Note also that such conditions should not rely on the result object (that is, the #result variable), as these are validated up-front to confirm the exclusion."

Polling Service:
package com.example.demo.config;

import org.springframework.cache.annotation.CachePut;
import org.springframework.stereotype.Service;

import java.util.concurrent.ThreadLocalRandom;

@Service
public class ExternalServiceClient {

    @CachePut(value = "weather", key = "#zip")
    public int getWeatherInfo(int zip){
        return ThreadLocalRandom.current().nextInt(60, 100);
    }

}

Biz Service:
package com.example.demo.config;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.cache.annotation.Cacheable;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Service;

import java.util.stream.IntStream;

@Service
public class WeatherService {

    @Autowired
    private ExternalServiceClient client;

    @Cacheable(value = "weather", key = "#zip")
    public int getInfo(int zip) {
        System.out.println("Weather Service api is called");
        return 0;
    }

    @Scheduled(fixedRate = 10_000)
    public void update() {
        System.out.println("updating weather");
        IntStream.rangeClosed(1, 5)
                .forEach(this.client::getWeatherInfo);
    }

}

Controller:
package com.example.demo.config;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.PathVariable;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;
import reactor.core.publisher.Mono;

@RestController
@RequestMapping("/weather")
public class WeatherController {

    @Autowired
    private WeatherService service;

    @GetMapping("/{zip}")
    public Mono<Integer> getWeather(@PathVariable int zip){
        return Mono.fromSupplier(() -> this.service.getInfo(zip));
    }

}
////////////////////////////////////////////////////////////////////////////////////////////

@CacheConfig annotation
So far we have seen that caching operations offered many customization options and these can be set on an operation basis. 

However, some of the customization options can be tedious to configure if they apply to all operations of the class. For instance, specifying the name of the cache to use for every cache operation of the class could be replaced by a single class-level definition.
This is where @CacheConfig comes into play.

@CacheConfig("books")
public class BookRepositoryImpl implements BookRepository {

    @Cacheable
    public Book findBook(ISBN isbn) {...}
}
/////////////////////////////////////////////////////////////////////////////////////////////

@Caching:

Use case , what if i want to apply different caching on single method
like having two different CacheEvit Policy.

Sometimes, multiple annotations of the same type (such as @CacheEvict or @CachePut) need to be specified — for example, because the condition or the key expression is different between different caches. @Caching lets multiple nested @Cacheable, @CachePut, and @CacheEvict annotations be used on the same method. The following example uses two @CacheEvict annotations

@Caching(evict = { @CacheEvict("primary"), @CacheEvict(cacheNames="secondary", key="#p0") })
public Book importBooks(String deposit, Date date)
............................................................................................
			SQL - Spring Boot -  Caching -Redis
............................................................................................

Steps:
pom.xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>2.4.1</version>
        <relativePath/> <!-- lookup parent from repository -->
    </parent>
    <groupId>com.example</groupId>
    <artifactId>mycache-app</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <name>mycache-app</name>
    <description>Demo project for Spring Boot</description>

    <properties>
        <java.version>1.8</java.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-cache</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-jpa</artifactId>
        </dependency>
        <dependency>
            <groupId>com.h2database</groupId>
            <artifactId>h2</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>
        <dependency>
            <groupId>org.redisson</groupId>
            <artifactId>redisson-spring-boot-starter</artifactId>
            <version>3.17.0</version>
        </dependency>
        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <optional>true</optional>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>

</project>

Application:

Main:
package com.example;

import com.example.entity.Book;
import com.example.repo.BookRepository;
import org.springframework.boot.CommandLineRunner;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cache.annotation.EnableCaching;
import org.springframework.context.annotation.Bean;

@SpringBootApplication
@EnableCaching
public class MycacheAppApplication {

    public static void main(String[] args) {
        SpringApplication.run(MycacheAppApplication.class, args);
    }

    @Bean
    CommandLineRunner runner(BookRepository bookRepository) {
        return args -> {
            Book book = null;
            for (int i = 0; i < 10; i++) {
                book = new Book();
                book.setAuthor("Author " + i);
                book.setCategory("Caching " + i);
                book.setEdition(i + "nd edition ");
                book.setName("Caching in Action " + i);
                book.setPublisher("my Publisher " + i);
                bookRepository.save(book);
            }
            bookRepository.findAll().forEach(System.out::println);

        };
    }
}

Reddison configuration:
package com.example;

import org.redisson.Redisson;
import org.redisson.api.RedissonClient;
import org.redisson.config.Config;
import org.redisson.spring.cache.CacheConfig;
import org.redisson.spring.cache.RedissonSpringCacheManager;
import org.springframework.cache.CacheManager;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

import java.util.HashMap;
import java.util.Map;

@Configuration
public class ReddisonConfig {
    @Bean(destroyMethod = "shutdown")
    RedissonClient redisson() {
        Config config = new Config();
        config.useSingleServer()
                .setAddress("redis://127.0.0.1:6379");
        return Redisson.create(config);
    }

    @Bean
    public CacheManager cacheManager(RedissonClient reddisonClient) {
        Map<String, CacheConfig> config = new HashMap<>();
        return new RedissonSpringCacheManager(redisson(), config);
    }
}




Controller:

package com.example.api;

import com.example.entity.Book;
import com.example.service.BookService;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.*;


@RestController
public class BooksController {

    @Autowired
    private BookService bookService;

    @PostMapping("/book")
    public Book addBook(@RequestBody Book book) {
        return bookService.addBook(book);
    }

    @PutMapping("/book")
    public Book updateBook(@RequestBody Book book) {
        return bookService.updateBook(book);
    }

    @GetMapping("/book/{id}")
    public Book getBook(@PathVariable long id) {
        return bookService.getBook(id);
    }

    @DeleteMapping("/book/{id}")
    public String deleteBook(@PathVariable long id) {
        return bookService.deleteBook(id);
    }
}
Entity:
package com.example.entity;

import javax.persistence.*;
import java.io.Serializable;

@Entity
public class Book implements  Serializable {
    private static final long serialVersionUID = 1307525040224585678L;
    @Id
    @GeneratedValue
    private long id;
    private String name;
    private String category;
    private String author;
    private String publisher;
    private String edition;

    public long getId() {
        return id;
    }

    public void setId(long id) {
        this.id = id;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getCategory() {
        return category;
    }

    public void setCategory(String category) {
        this.category = category;
    }

    public String getAuthor() {
        return author;
    }

    public void setAuthor(String author) {
        this.author = author;
    }

    public String getPublisher() {
        return publisher;
    }

    public void setPublisher(String publisher) {
        this.publisher = publisher;
    }

    public String getEdition() {
        return edition;
    }

    public void setEdition(String edition) {
        this.edition = edition;
    }

    @Override
    public String toString() {
        return "Book{" +
                "id=" + id +
                ", name='" + name + '\'' +
                ", category='" + category + '\'' +
                ", author='" + author + '\'' +
                ", publisher='" + publisher + '\'' +
                ", edition='" + edition + '\'' +
                '}';
    }
}

Repository:
package com.example.repo;
import com.example.entity.Book;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import javax.transaction.Transactional;

public interface BookRepository extends JpaRepository<Book, Long> {
    @Transactional
    @Modifying
    @Query("update Book u set u.name=?2 where u.id=?1")
    int updateAddress(long id, String name);
}

Service:
package com.example.service;


import com.example.entity.Book;

public interface BookService {
    Book addBook(Book book);

    Book updateBook(Book book);

    Book getBook(long id);

    String deleteBook(long id);
}

Service with CacheAnnoation:
package com.example.service;

import com.example.entity.Book;
import com.example.repo.BookRepository;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.cache.annotation.CacheConfig;
import org.springframework.cache.annotation.CacheEvict;
import org.springframework.cache.annotation.CachePut;
import org.springframework.cache.annotation.Cacheable;
import org.springframework.stereotype.Service;

import java.util.Optional;

@Service
@CacheConfig(cacheNames = "books")
public class BookServiceImpl implements BookService {

    private static final Logger logger = LoggerFactory.getLogger(BookServiceImpl.class);
    @Autowired
    private BookRepository bookRepository;

    @Override
    public Book addBook(Book book) {
        logger.info("adding book with id - {}", book.getId());
        return bookRepository.save(book);
    }


    @Override
    @CachePut(key = "#book.id")
    public Book updateBook(Book book) {
        bookRepository.updateAddress(book.getId(), book.getName());
        logger.info("book updated with new name");
        return book;
    }

    //   @Override
    //now this method will executed only if cache miss, if cache hit, method wont be executed
//    @Cacheable(cacheNames = {"books", "isbns"})
//    public Book getBook(long id) {
//        logger.info("fetching book from db");
//        Optional<Book> book = bookRepository.findById(id);
//        if (book.isPresent()) {
//            return book.get();
//        } else {
//            return new Book();
//        }
//    }

    @Override
    @Cacheable(key = "#id")
    public Book getBook(long id) {
        logger.info("fetching book from db");
        Optional<Book> book = bookRepository.findById(id);
        if (book.isPresent()) {
            return book.get();
        } else {
            return new Book();
        }
    }
    //conditional : cache only books whose id greater than 5
//    @Override
//    @Cacheable(cacheNames = "books", condition = "#id>5 ")
//    public Book getBook(long id) {
//        logger.info("fetching book from db" + id);
//        Optional<Book> book = bookRepository.findById(id);
//        if (book.isPresent()) {
//            return book.get();
//        } else {
//            return new Book();
//        }
//    }

//    @Override
//    @Cacheable(cacheNames = "books",  unless = "#id==3")
//    public Book getBook(long id) {
//        logger.info("fetching book from db" + id);
//        Optional<Book> book = bookRepository.findById(id);
//        if (book.isPresent()) {
//            return book.get();
//        } else {
//            return new Book();
//        }
//    }

    @Override
    @CacheEvict(key = "#id", allEntries = true)
    public String deleteBook(long id) {
        bookRepository.deleteById(id);
        return "Book deleted";
    }
}
.............................................&&&&&&.........................................

Replication
Persistency
Cluster












